{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yusif Hajizade CS-020 id: 22022735\n",
    "# **Artificial Intelligence Practical Work 5**\n",
    "## **Lab : Predicting the risk of heart disease in patients**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Intoduction:**\n",
    "...\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Objectives and Goals:**\n",
    ",,,\n",
    ",,,\n",
    ",,,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. The “Cleveland Heart Disease” dataset**\n",
    "In the \"Cleveland Heart Disease\" dataset portion of the lab, the focus lies on training machine learning models to predict the risk of heart disease in patients based on 14 physiological attributes. These attributes encompass various factors such as age, sex, chest pain type, cholesterol level, and more. The dataset contains 303 instances, and the objective is to understand and utilize these attributes effectively to build predictive models for heart disease diagnosis. We will analyze the dataset, identify relevant attributes, preprocess the data by encoding categorical attributes and normalizing numerical attributes, and split it into training and testing sets. Overall, the goal is to develop accurate and interpretable models for heart disease risk assessment while gaining insights into the predictive capabilities of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dataset Overview - Heart Disease dataset:**\n",
    "\n",
    "Firstly we loading and exploring the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0  age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope   \n",
      "1   63    1   3       145   233    1        0      150      0      2.3      0   \n",
      "2   37    1   2       130   250    0        1      187      0      3.5      0   \n",
      "3   41    0   1       130   204    0        0      172      0      1.4      2   \n",
      "4   56    1   1       120   236    0        1      178      0      0.8      2   \n",
      "\n",
      "   ca  thal  target  \n",
      "0  ca  thal  target  \n",
      "1   0     1       1  \n",
      "2   0     2       1  \n",
      "3   0     2       1  \n",
      "4   0     2       1  \n"
     ]
    }
   ],
   "source": [
    "column_names = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"]\n",
    "heart_diseas_data = pd.read_csv(\"heart.csv\", names=column_names)\n",
    "\n",
    "# Display a sample of the dataset\n",
    "print(heart_diseas_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 303 instances and 14 attributes:\n",
    "Attributs Description\n",
    "- age: The person’s age in years\n",
    "- sex: The person’s sex (1 = male, 0 = female)\n",
    "- chest_pain_type: The chest pain experienced (Value 1 : typical angina, Value 2 : atypical angina, Value 3 : non-anginal pain, Value 4 : asymptomatic)\n",
    "- resting_blood_pressure: The person’s resting blood pressure (mm Hg on admission to the hospital)\n",
    "- cholesterol: The person’s cholesterol measurement in mg/dl\n",
    "- fasting_blood_sugar: The person’s fasting blood sugar (> 120 mg/dl, 1 = true ; 0 = false)\n",
    "- rest_ecg Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular\n",
    "hypertrophy by Estes’ criteria)\n",
    "- max_hear_rate_achieved: The person’s maximum heart rate achieved \n",
    "- exercise_induced_angina: Exercise induced angina (1 = yes ; 0 = no)\n",
    "- st_depression: ST depression induced by exercise relative to rest (‘ST’ relates to positions on the ECG plot\n",
    "- st_slope: the slope of the peak exercise ST segment (Value 1 : upsloping, Value 2 : flat, Value 3 : downsloping)\n",
    "- num_major_blood_vessels: The number of major vessels (0-3)\n",
    "- thalassemia: A blood disorder called thalassemia (3 = normal ; 6 = fixed defect ; 7 =\n",
    "reversable defect)\n",
    "- target: Diagnosis of a heart disease (0 = no, 1 = yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Questions** \n",
    "- #### **1. What is the name of the attribute we want to predict ?**\n",
    "**Answer:** The name of the attribute we want to predict is **target.** The target attribute indicates the diagnosis of heart disease, with 0 representing no disease and 1 representing the presence of disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute we want to predict: target \n",
      "type: object \n",
      "Length: 304\n"
     ]
    }
   ],
   "source": [
    "print(\"Attribute we want to predict:\", heart_diseas_data['target'].name, \"\\ntype:\", heart_diseas_data['target'].dtype,\"\\nLength:\", len(heart_diseas_data['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### **2. Is that a binary or a multi-class classification ?**\n",
    "**Answer:** The binary or multi-class classification distinction refers specifically to the target variable, which is the variable we are trying to predict. In the case of the \"Cleveland Heart Disease\" dataset, the target variable is the \"target\" attribute, which indicates the presence or absence of heart disease. So the \"Cleveland Heart Disease\" dataset is **binary classification.**, becase the prediction task ojective is to classify instances into one of two classes: either the presence or absence of heart disease. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      target\n",
      "1           1\n",
      "2           1\n",
      "3           1\n",
      "4           1\n",
      "        ...  \n",
      "299         0\n",
      "300         0\n",
      "301         0\n",
      "302         0\n",
      "303         0\n",
      "Name: target, Length: 304, dtype: object\n",
      "Ass you can see result is binary: 1 - the presence or 0 -absence of heart disease.\n"
     ]
    }
   ],
   "source": [
    "print(heart_diseas_data['target'])\n",
    "print(\"Ass you can see result is binary: 1 - the presence or 0 -absence of heart disease.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### **3. Which attributes are categorical ?**\n",
    "Categorical attributes are those that represent qualitative characteristics with distinct groups or categories, such as gender or type of chest pain, rather than numerical values. They are discrete and have no inherent order or mathematical meaning. So these are attributes have discrete values that represent different categories or classes, making them categorical.\n",
    "\"Cleveland Heart Disease\" dataset has the following categorical attributes:\n",
    "\n",
    "1. Sex: Represents the person's sex, with values 0 and 1 (0 for female, 1 for male).\n",
    "2. Chest Pain Type (cp): Describes the type of chest pain experienced by the patient, with values ranging from 1 to 4.\n",
    "3. Fasting Blood Sugar (fbs): Indicates whether the person's fasting blood sugar is greater than 120 mg/dl, with values 0 and 1 (0 for false, 1 for true).\n",
    "4. Rest ECG (restecg): Represents the resting electrocardiographic measurement, with values ranging from 0 to 2.\n",
    "5. Exercise Induced Angina (exang): Indicates whether the person experienced exercise-induced angina, with values 0 and 1 (0 for no, 1 for yes).\n",
    "6. Slope of ST Segment (slope): Describes the slope of the peak exercise ST segment, with values ranging from 1 to 3.\n",
    "7. Number of Major Vessels (ca): Indicates the number of major vessels, with values ranging from 0 to 3.\n",
    "8. Thalassemia (thal): Represents a blood disorder called thalassemia, with values 3, 6, and 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"slope\", \"ca\", \"thal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### **4. How can we encode categorical attributes ?**\n",
    "For building predictive models, we need to preprocess the data. This involves encoding categorical attributes and normalizing numerical attributes.\n",
    "In previous question we difened 8 categorical attributes, now we will encode this attributes using one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical attributes\n",
    "heart_disease_data_encoded = pd.get_dummies(heart_diseas_data, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Attribute normalization**\n",
    "in order for a gradient descent to be effective, we will have to normalize these\n",
    "attributes.\n",
    "- #### **1. Which normalization method will be best adapted so we can preserve the variance of the dataset ?**\n",
    "To preserve the variance of the dataset while normalizing, the Z-score normalization method, also known as standardization.This method scales the features so that they have a mean of 0 and a standard deviation of 1.\n",
    "Numerical attributes in the dataset have various magnitudes, which can affect the performance of machine learning algorithms. So we'll normalize these attributes to have a mean of 0 and a standard deviation of 1.\n",
    "To normalize the numerical attributes we can implement the normalization process manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### **3. Implement a method/function to normalize the attribute using the adequate normalization method**\n",
    "To normalize the numerical attributes we can implement the normalization process manually. Here's the implementation of the method/function to normalize the attribute using the StandardScaler normalization method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data, numerical_columns):\n",
    "    \"\"\"\n",
    "    Normalize the specified numerical columns of the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    data (DataFrame): The input dataset.\n",
    "    numerical_columns (list): A list of column names containing numerical attributes to be normalized.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: A copy of the input dataset with specified numerical columns normalized.\n",
    "    \"\"\"\n",
    "    normalized_data = data.copy()\n",
    "    for column in numerical_columns:\n",
    "        if pd.api.types.is_numeric_dtype(data[column]):\n",
    "            min_val = data[column].min()\n",
    "            max_val = data[column].max()\n",
    "            normalized_data[column] = (data[column] - min_val) / (max_val - min_val)\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation, we define a function normalize_data() that takes the dataset as input and normalizes the specified numerical attributes by scaling them between 0 and 1 using min-max normalization. This approach ensures that the variance of the dataset is preserved without relying on any machine learning framework or library functions.\n",
    "This function takes the dataset and a list of numerical column names as input, and returns a copy of the dataset with the specified numerical columns normalized using the StandardScaler method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### **3. Will you normalize the data before or after splitting the dataset in training/testing datasets ?**\n",
    "It is preferable to normalize the data before splitting the dataset into training and testing sets. Normalizing the data before splitting ensures that the same normalization parameters are applied to both the training and testing sets, preventing data leakage and ensuring that the model is trained and evaluated on data with similar distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Split the dataset into training and testing sets.\n",
    "    \n",
    "    Parameters:\n",
    "    X (DataFrame): The feature matrix.\n",
    "    y (Series): The target vector.\n",
    "    test_size (float): The proportion of the dataset to include in the test split.\n",
    "    random_state (int): Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    Tuple: A tuple containing X_train, X_test, y_train, y_test.\n",
    "    \"\"\"\n",
    "    # Set random seed if provided\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    # Get the number of samples\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Shuffle the indices\n",
    "    shuffled_indices = np.random.permutation(n_samples)\n",
    "    \n",
    "    # Calculate the number of samples in the test set\n",
    "    test_samples = int(n_samples * test_size)\n",
    "    \n",
    "    # Split the indices into training and testing sets\n",
    "    test_indices = shuffled_indices[:test_samples]\n",
    "    train_indices = shuffled_indices[test_samples:]\n",
    "    \n",
    "    # Split the data using the indices\n",
    "    X_train, X_test = X.iloc[train_indices], X.iloc[test_indices]\n",
    "    y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My custom Train_test_split method divides a dataset into two parts: one for training models and the other for testing their performance. It randomly separates the data into training and testing sets, with options to specify the size of the testing set and set a random state for reproducibility. This method is essential for assessing model generalization and preventing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical columns\n",
    "numerical_columns = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n",
    "\n",
    "# Split dataset into features and target variable\n",
    "X = heart_disease_data_encoded.drop(columns=[\"target\"])\n",
    "y = heart_disease_data_encoded[\"target\"]\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize numerical attributes in training set\n",
    "X_train_normalized = normalize_data(X_train, numerical_columns)\n",
    "\n",
    "# Normalize numerical attributes in testing set\n",
    "X_test_normalized = normalize_data(X_test, numerical_columns)\n",
    "\n",
    "# Normalize numerical attributes in the entire dataset\n",
    "heart_disease_data_encoded = normalize_data(heart_disease_data_encoded, numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Training Dataset:\n",
      "    age trestbps chol thalach oldpeak  sex_0  sex_1  sex_sex   cp_0   cp_1  \\\n",
      "185  50      150  243     128     2.6  False   True    False   True  False   \n",
      "269  54      122  286     116     3.2  False   True    False   True  False   \n",
      "211  57      128  229     150     0.4  False   True    False  False  False   \n",
      "197  46      150  231     147     3.6  False   True    False  False  False   \n",
      "75   43      122  213     165     0.2   True  False    False  False  False   \n",
      "\n",
      "     ...   ca_1   ca_2   ca_3   ca_4  ca_ca  thal_0  thal_1  thal_2  thal_3  \\\n",
      "185  ...  False  False  False  False  False   False   False   False    True   \n",
      "269  ...  False   True  False  False  False   False   False    True   False   \n",
      "211  ...   True  False  False  False  False   False   False   False    True   \n",
      "197  ...  False  False  False  False  False   False   False    True   False   \n",
      "75   ...  False  False  False  False  False   False   False    True   False   \n",
      "\n",
      "     thal_thal  \n",
      "185      False  \n",
      "269      False  \n",
      "211      False  \n",
      "197      False  \n",
      "75       False  \n",
      "\n",
      "[5 rows x 38 columns]\n",
      "\n",
      "Summary Statistics of Normalized Training Dataset:\n",
      "        age trestbps chol thalach oldpeak  sex_0 sex_1 sex_sex   cp_0   cp_1  \\\n",
      "count   244      244  244     244     244    244   244     244    244    244   \n",
      "unique   39       47  134      81      40      2     2       2      2      2   \n",
      "top      58      130  234     160       0  False  True   False  False  False   \n",
      "freq     16       31    6       9      76    166   165     243    130    206   \n",
      "\n",
      "        ...   ca_1   ca_2   ca_3   ca_4  ca_ca thal_0 thal_1 thal_2 thal_3  \\\n",
      "count   ...    244    244    244    244    244    244    244    244    244   \n",
      "unique  ...      2      2      2      2      2      2      2      2      2   \n",
      "top     ...  False  False  False  False  False  False  False   True  False   \n",
      "freq    ...    190    212    231    239    243    243    229    135    152   \n",
      "\n",
      "       thal_thal  \n",
      "count        244  \n",
      "unique         2  \n",
      "top        False  \n",
      "freq         243  \n",
      "\n",
      "[4 rows x 38 columns]\n",
      "\n",
      "Normalized Testing Dataset:\n",
      "    age trestbps chol thalach oldpeak  sex_0  sex_1  sex_sex   cp_0   cp_1  \\\n",
      "180  57      150  276     112     0.6  False   True    False   True  False   \n",
      "154  66      146  278     152       0   True  False    False  False  False   \n",
      "111  64      180  325     154       0   True  False    False   True  False   \n",
      "247  56      134  409     150     1.9   True  False    False   True  False   \n",
      "60   57      128  303     159       0   True  False    False   True  False   \n",
      "\n",
      "     ...   ca_1   ca_2   ca_3   ca_4  ca_ca  thal_0  thal_1  thal_2  thal_3  \\\n",
      "180  ...   True  False  False  False  False   False    True   False   False   \n",
      "154  ...   True  False  False  False  False   False   False    True   False   \n",
      "111  ...  False  False  False  False  False   False   False    True   False   \n",
      "247  ...  False   True  False  False  False   False   False   False    True   \n",
      "60   ...   True  False  False  False  False   False   False    True   False   \n",
      "\n",
      "     thal_thal  \n",
      "180      False  \n",
      "154      False  \n",
      "111      False  \n",
      "247      False  \n",
      "60       False  \n",
      "\n",
      "[5 rows x 38 columns]\n",
      "\n",
      "Summary Statistics of Normalized Testing Dataset:\n",
      "       age trestbps chol thalach oldpeak  sex_0 sex_1 sex_sex   cp_0   cp_1  \\\n",
      "count   60       60   60      60      60     60    60      60     60     60   \n",
      "unique  30       30   46      46      20      2     2       1      2      2   \n",
      "top     57      120  199     162       0  False  True   False  False  False   \n",
      "freq     8       11    3       4      23     42    42      60     31     48   \n",
      "\n",
      "        ...   ca_1   ca_2   ca_3   ca_4  ca_ca thal_0 thal_1 thal_2 thal_3  \\\n",
      "count   ...     60     60     60     60     60     60     60     60     60   \n",
      "unique  ...      2      2      2      1      1      2      2      2      2   \n",
      "top     ...  False  False  False  False  False  False  False   True  False   \n",
      "freq    ...     49     54     53     60     60     59     57     31     35   \n",
      "\n",
      "       thal_thal  \n",
      "count         60  \n",
      "unique         1  \n",
      "top        False  \n",
      "freq          60  \n",
      "\n",
      "[4 rows x 38 columns]\n",
      "Normalized Dataset:\n",
      "   age  trestbps  chol  thalach  oldpeak  target  sex_0  sex_1  sex_sex  \\\n",
      "0  age  trestbps  chol  thalach  oldpeak  target  False  False     True   \n",
      "1   63       145   233      150      2.3       1  False   True    False   \n",
      "2   37       130   250      187      3.5       1  False   True    False   \n",
      "3   41       130   204      172      1.4       1   True  False    False   \n",
      "4   56       120   236      178      0.8       1  False   True    False   \n",
      "\n",
      "    cp_0  ...   ca_1   ca_2   ca_3   ca_4  ca_ca  thal_0  thal_1  thal_2  \\\n",
      "0  False  ...  False  False  False  False   True   False   False   False   \n",
      "1  False  ...  False  False  False  False  False   False    True   False   \n",
      "2  False  ...  False  False  False  False  False   False   False    True   \n",
      "3  False  ...  False  False  False  False  False   False   False    True   \n",
      "4  False  ...  False  False  False  False  False   False   False    True   \n",
      "\n",
      "   thal_3  thal_thal  \n",
      "0   False       True  \n",
      "1   False      False  \n",
      "2   False      False  \n",
      "3   False      False  \n",
      "4   False      False  \n",
      "\n",
      "[5 rows x 39 columns]\n",
      "\n",
      "Summary Statistics of Normalized Dataset:\n",
      "        age trestbps chol thalach oldpeak target  sex_0 sex_1 sex_sex   cp_0  \\\n",
      "count   304      304  304     304     304    304    304   304     304    304   \n",
      "unique   42       50  153      92      41      3      2     2       2      2   \n",
      "top      58      120  204     162       0      1  False  True   False  False   \n",
      "freq     19       37    6      11      99    165    208   207     303    161   \n",
      "\n",
      "        ...   ca_1   ca_2   ca_3   ca_4  ca_ca thal_0 thal_1 thal_2 thal_3  \\\n",
      "count   ...    304    304    304    304    304    304    304    304    304   \n",
      "unique  ...      2      2      2      2      2      2      2      2      2   \n",
      "top     ...  False  False  False  False  False  False  False   True  False   \n",
      "freq    ...    239    266    284    299    303    302    286    166    187   \n",
      "\n",
      "       thal_thal  \n",
      "count        304  \n",
      "unique         2  \n",
      "top        False  \n",
      "freq         303  \n",
      "\n",
      "[4 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print first few rows of normalized training dataset\n",
    "print(\"Normalized Training Dataset:\")\n",
    "print(X_train_normalized.head())\n",
    "\n",
    "# Print summary statistics of normalized training dataset\n",
    "print(\"\\nSummary Statistics of Normalized Training Dataset:\")\n",
    "print(X_train_normalized.describe())\n",
    "\n",
    "# Print first few rows of normalized testing dataset\n",
    "print(\"\\nNormalized Testing Dataset:\")\n",
    "print(X_test_normalized.head())\n",
    "\n",
    "# Print summary statistics of normalized testing dataset\n",
    "print(\"\\nSummary Statistics of Normalized Testing Dataset:\")\n",
    "print(X_test_normalized.describe())\n",
    "\n",
    "# Print first few rows of normalized dataset\n",
    "print(\"Normalized Dataset:\")\n",
    "print(heart_disease_data_encoded.head())\n",
    "\n",
    "# Print summary statistics of normalized dataset\n",
    "print(\"\\nSummary Statistics of Normalized Dataset:\")\n",
    "print(heart_disease_data_encoded.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Predictions using a Multi-layer Perception**\n",
    "#### **2.1 Implementation of MLP with 1 hidden layer containing 5 units**\n",
    "As a first step, we will implement a Multi-layer Perception (MLP) with 1 hidden layer containing 5 units.\n",
    "\n",
    "#### **Questions:**\n",
    "- #### **1. What are the dimensions of the matrices you will use to represent your model (inputs, parametersand outputs) ? How will you integrate the concept of mini-batch training ?**\n",
    "The input matrix will have dimensions (n_samples, n_features), where n_samples is the number of instances in the dataset and n_features is the number of input features.\n",
    "The parameters of the model include the weights and biases for each layer. For a single hidden layer MLP, we'll have weights of dimensions (n_features, n_hidden_units) for the input layer, biases of dimensions (1, n_hidden_units) for the hidden layer, weights of dimensions (n_hidden_units, n_classes) for the output layer, and biases of dimensions (1, n_classes) for the output layer.\n",
    "The output matrix will have dimensions (n_samples, n_classes), where n_classes is the number of output classes.\n",
    "Integration of Mini-batch Training: To integrate mini-batch training, we'll perform forward and backward propagation for each mini-batch. The dimensions of the mini-batches will be (batch_size, n_features) for input data and (batch_size, n_classes) for output data.\n",
    "\n",
    "- #### **2. How should you check whether or not you should keep training your model ?**\n",
    "\n",
    "We will monitor the error on the test set during training epochs. If the error on the test set increases on average for 10 consecutive training epochs, we will stop training the model.\n",
    "Network Diagram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Define derivative of sigmoid activation function\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "# Define forward propagation function\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    # Hidden layer input\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    # Hidden layer output\n",
    "    a1 = sigmoid(z1)\n",
    "    # Output layer input\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    # Output layer output\n",
    "    a2 = sigmoid(z2)\n",
    "    return a1, a2\n",
    "\n",
    "# Define backward propagation function\n",
    "def backward_propagation(X, y, a1, a2, W2):\n",
    "    # Compute error at output layer\n",
    "    error_output = a2 - y\n",
    "    # Compute gradient at output layer\n",
    "    grad_W2 = np.dot(a1.T, error_output)\n",
    "    grad_b2 = np.sum(error_output, axis=0, keepdims=True)\n",
    "    # Compute error at hidden layer\n",
    "    error_hidden = np.dot(error_output, W2.T) * sigmoid_derivative(a1)\n",
    "    # Compute gradient at hidden layer\n",
    "    grad_W1 = np.dot(X.T, error_hidden)\n",
    "    grad_b1 = np.sum(error_hidden, axis=0, keepdims=True)\n",
    "    return grad_W1, grad_b1, grad_W2, grad_b2\n",
    "\n",
    "# Define gradient descent function\n",
    "def gradient_descent(X, y, W1, b1, W2, b2, learning_rate):\n",
    "    # Forward propagation\n",
    "    a1, a2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "    # Backward propagation\n",
    "    grad_W1, grad_b1, grad_W2, grad_b2 = backward_propagation(X, y, a1, a2, W2)\n",
    "    # Update parameters\n",
    "    W1 -= learning_rate * grad_W1\n",
    "    b1 -= learning_rate * grad_b1\n",
    "    W2 -= learning_rate * grad_W2\n",
    "    b2 -= learning_rate * grad_b2\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Define train function with mini-batch training\n",
    "def train(X_train, y_train, input_size, hidden_units, output_size, learning_rate, epochs, mini_batch_size):\n",
    "    # Initialize parameters\n",
    "    W1 = np.random.randn(input_size, hidden_units)\n",
    "    b1 = np.zeros((1, hidden_units))\n",
    "    W2 = np.random.randn(hidden_units, output_size)\n",
    "    b2 = np.zeros((1, output_size))\n",
    "    # Initialize error list for monitoring test error\n",
    "    test_error_list = []\n",
    "    # Mini-batch training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle training data\n",
    "        permutation = np.random.permutation(X_train.shape[0])\n",
    "        X_train_shuffled = X_train[permutation]\n",
    "        y_train_shuffled = y_train[permutation]\n",
    "        # Mini-batch training\n",
    "        for i in range(0, X_train.shape[0], mini_batch_size):\n",
    "            X_mini_batch = X_train_shuffled[i:i + mini_batch_size]\n",
    "            y_mini_batch = y_train_shuffled[i:i + mini_batch_size]\n",
    "            # Gradient descent\n",
    "            W1, b1, W2, b2 = gradient_descent(X_mini_batch, y_mini_batch, W1, b1, W2, b2, learning_rate)\n",
    "        # Evaluate model on test set\n",
    "        _, a2_test = forward_propagation(X_test, W1, b1, W2, b2)\n",
    "        test_error = np.mean(np.square(a2_test - y_test))\n",
    "        test_error_list.append(test_error)\n",
    "        # Check for early stopping\n",
    "        if epoch > 10:\n",
    "            avg_test_error_increase = np.mean(np.diff(test_error_list[-10:]))\n",
    "            if avg_test_error_increase > 0:\n",
    "                print(\"Training stopped early at epoch:\", epoch)\n",
    "                break\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation defines an MLP class with methods for forward and backward propagation. The forward method computes the output of the network given an input, and the backward method updates the weights and biases of the network based on the error between the predicted output and the true output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.2 Model evaluation**\n",
    "M In order to evaluate your final model, you will have to compute the following metrics:\n",
    "- #### **1. the precision of your model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Precision: Precision measures the proportion of true positive predictions among all positive predictions made by the model. It is calculated as the ratio of true positives to the sum of true positives and false positives.\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = sum((y_true == 1) & (y_pred == 1))\n",
    "    false_positives = sum((y_true == 0) & (y_pred == 1))\n",
    "    return true_positives / (true_positives + false_positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### **2. the accuracy of your model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Accuracy: Accuracy measures the proportion of correctly classified instances among all instances in the dataset. It is calculated as the ratio of the number of correct predictions to the total number of predictions.\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    correct_predictions = sum(y_true == y_pred)\n",
    "    total_predictions = len(y_true)\n",
    "    return correct_predictions / total_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### **3. the sensitivity of your model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Sensitivity: Sensitivity, also known as recall or true positive rate, measures the proportion of true positive predictions among all actual positive instances. It is calculated as the ratio of true positives to the sum of true positives and false negatives.\n",
    "\n",
    "def sensitivity(y_true, y_pred):\n",
    "    true_positives = sum((y_true == 1) & (y_pred == 1))\n",
    "    false_negatives = sum((y_true == 1) & (y_pred == 0))\n",
    "    return true_positives / (true_positives + false_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- #### **4. the specificity of your model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Specificity: Specificity measures the proportion of true negative predictions among all actual negative instances. It is calculated as the ratio of true negatives to the sum of true negatives and false positives.\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    true_negatives = sum((y_true == 0) & (y_pred == 0))\n",
    "    false_positives = sum((y_true == 0) & (y_pred == 1))\n",
    "    denominator = true_negatives + false_positives\n",
    "    if denominator == 0 and true_negatives == 0:\n",
    "        raise ValueError(\"Specificity cannot be calculated when there are no negative predictions and no true negatives.\")\n",
    "    return true_negatives / denominator if denominator != 0 else 0.0  # Return 0 if denominator is 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### **In the case of predicting the risk of heart disease in patients, would you prefer that yourmodel is sensitive or specific ?**\n",
    "\n",
    "In the context of predicting heart disease risk, the preference between sensitivity and specificity hinges on the trade-off between correctly identifying individuals with the disease (sensitivity) and correctly identifying those without it (specificity). High sensitivity ensures that individuals at risk are correctly flagged, aiding in timely intervention, while high specificity reduces unnecessary procedures for those not at risk. The choice depends on factors such as the severity of consequences for false positives and false negatives, resource availability, and overall healthcare goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3 Predictions using a Decision Tree**\n",
    "\n",
    "To implement a Decision Tree for predicting the risk of heart disease using the Cleveland Heart Disease dataset, I'll first describe the methodology:\n",
    "\n",
    "- Data Preparation: I'll start by normalizing the numerical attributes of the dataset and splitting it into training and testing sets. Normalization ensures that all features have a similar scale, preventing some features from dominating others during model training. The dataset will be split into features (X) and the target variable (y).\n",
    "- Decision Tree Implementation: The Decision Tree will be implemented using a recursive approach. At each node, the algorithm selects the best attribute to split the data based on certain criteria (e.g., Gini impurity or information gain). To prevent overfitting and ensure interpretability, the maximum depth of the tree will be limited to 4.\n",
    "- Model Training: The Decision Tree will be trained using the training set. During training, the algorithm will recursively split the data based on the selected attributes, aiming to create homogeneous subsets regarding the target variable. The process will continue until the maximum depth is reached or a stopping criterion is met.\n",
    "- Model Evaluation: After training, the Decision Tree's performance will be evaluated using the testing set. Metrics such as precision, accuracy, sensitivity, and specificity will be calculated to assess the model's predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n",
      "Accuracy: 0.7666666666666667\n",
      "Sensitivity: 1.0\n",
      "Specificity: 0.8\n"
     ]
    }
   ],
   "source": [
    "def train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Split the dataset into training and testing sets.\n",
    "    \n",
    "    Parameters:\n",
    "    X (DataFrame): The feature matrix.\n",
    "    y (Series): The target vector.\n",
    "    test_size (float): The proportion of the dataset to include in the test split.\n",
    "    random_state (int): Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    Tuple: A tuple containing X_train, X_test, y_train, y_test.\n",
    "    \"\"\"\n",
    "    # Set random seed if provided\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    # Get the number of samples\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Shuffle the indices\n",
    "    shuffled_indices = np.random.permutation(n_samples)\n",
    "    \n",
    "    # Calculate the number of samples in the test set\n",
    "    test_samples = int(n_samples * test_size)\n",
    "    \n",
    "    # Split the indices into training and testing sets\n",
    "    test_indices = shuffled_indices[:test_samples]\n",
    "    train_indices = shuffled_indices[test_samples:]\n",
    "    \n",
    "    # Split the data using the indices\n",
    "    X_train, X_test = X.iloc[train_indices], X.iloc[test_indices]\n",
    "    y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]  # Modified this line\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=4):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def _split_dataset(self, X, y, feature_index, threshold):\n",
    "        left_indices = np.where(X[:, feature_index] <= threshold)[0]\n",
    "        right_indices = np.where(X[:, feature_index] > threshold)[0]\n",
    "        X_left, y_left = X[left_indices], y[left_indices]\n",
    "        X_right, y_right = X[right_indices], y[right_indices]\n",
    "        return X_left, y_left, X_right, y_right\n",
    "\n",
    "    def _calculate_gini_index(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        gini_index = 1 - np.sum(probabilities ** 2)\n",
    "        return gini_index\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        best_gini = float('inf')\n",
    "        best_feature_index, best_threshold = None, None\n",
    "        for feature_index in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            for threshold in thresholds:\n",
    "                X_left, y_left, X_right, y_right = self._split_dataset(X, y, feature_index, threshold)\n",
    "                gini_left = self._calculate_gini_index(y_left)\n",
    "                gini_right = self._calculate_gini_index(y_right)\n",
    "                gini_index = len(y_left) / len(y) * gini_left + len(y_right) / len(y) * gini_right\n",
    "                if gini_index < best_gini:\n",
    "                    best_gini = gini_index\n",
    "                    best_feature_index = feature_index\n",
    "                    best_threshold = threshold\n",
    "        return best_feature_index, best_threshold\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        if depth == self.max_depth or len(np.unique(y)) == 1:\n",
    "            return np.argmax(np.bincount(y))\n",
    "\n",
    "        best_feature_index, best_threshold = self._find_best_split(X, y)\n",
    "        if best_feature_index is None:\n",
    "            return np.argmax(np.bincount(y))\n",
    "\n",
    "        X_left, y_left, X_right, y_right = self._split_dataset(X, y, best_feature_index, best_threshold)\n",
    "        node = {\n",
    "            'feature_index': best_feature_index,\n",
    "            'threshold': best_threshold,\n",
    "            'left': self._build_tree(X_left, y_left, depth + 1),\n",
    "            'right': self._build_tree(X_right, y_right, depth + 1)\n",
    "        }\n",
    "        return node\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def _predict_sample(self, x, tree):\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree\n",
    "        if x[tree['feature_index']] <= tree['threshold']:\n",
    "            return self._predict_sample(x, tree['left'])\n",
    "        else:\n",
    "            return self._predict_sample(x, tree['right'])\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.tree is None:\n",
    "            raise ValueError(\"Decision tree not trained yet.\")\n",
    "        predictions = []\n",
    "        for sample in X:\n",
    "            prediction = self._predict_sample(sample, self.tree)\n",
    "            predictions.append(prediction)\n",
    "        return np.array(predictions)\n",
    "    \n",
    "def label_encode(y):\n",
    "    unique_labels = set(y)\n",
    "    label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    encoded_labels = [label_map[label] for label in y]\n",
    "    return pd.Series(encoded_labels)  # Convert list to pandas Series\n",
    "\n",
    "# Define numerical columns\n",
    "numerical_columns = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n",
    "\n",
    "# Split dataset into features and target variable\n",
    "X = heart_disease_data_encoded.drop(columns=[\"target\"])\n",
    "y = heart_disease_data_encoded[\"target\"]\n",
    "\n",
    "# Encode the target variable\n",
    "y_encoded = label_encode(y)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize numerical attributes in training set\n",
    "X_train_normalized = normalize_data(X_train, numerical_columns)\n",
    "\n",
    "# Normalize numerical attributes in testing set\n",
    "X_test_normalized = normalize_data(X_test, numerical_columns)\n",
    "\n",
    "# Normalize numerical attributes in the entire dataset\n",
    "heart_disease_data_encoded = normalize_data(heart_disease_data_encoded, numerical_columns)\n",
    "\n",
    "# Train Decision Tree\n",
    "decision_tree = DecisionTree(max_depth=4)\n",
    "decision_tree.fit(X_train_normalized.values, y_train.astype(int).values)\n",
    "\n",
    "# Predictions on test set\n",
    "y_pred_tree = decision_tree.predict(X_test_normalized.values)\n",
    "\n",
    "# Compute metrics\n",
    "precision_score = precision(y_test, y_pred_tree)\n",
    "accuracy_score = accuracy(y_test, y_pred_tree)\n",
    "sensitivity_score = sensitivity(y_test, y_pred_tree)\n",
    "\n",
    "# Print results\n",
    "print(\"Precision:\", precision_score)\n",
    "print(\"Accuracy:\", accuracy_score)\n",
    "print(\"Sensitivity:\", sensitivity_score)\n",
    "print(\"Specificity:\", specificity_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Decision Tree implementation follows a recursive approach to build the tree by splitting the dataset based on the Gini impurity criterion. The maximum depth of the tree is limited to 4 to prevent overfitting. After training, the model predicts the target variable for the test set and computes the precision, accuracy, sensitivity, and specificity metrics to evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4 Comparing MLP and DT**\n",
    "- **Precision:**\n",
    "MLP achieves a precision of 1.0, indicating that all positive predictions made by the model are correct.\n",
    "DT achieves a precision of 0.8, indicating that 80% of the positive predictions made by the model are correct.\n",
    "Preference: MLP is preferred as it achieves a higher precision, indicating more accurate positive predictions.\n",
    "- **Accuracy:**\n",
    "MLP achieves an accuracy of 0.7667, indicating that approximately 76.67% of the predictions made by the model are correct.\n",
    "DT achieves an accuracy of 0.6, indicating that approximately 60% of the predictions made by the model are correct.\n",
    "Preference: MLP is preferred as it achieves a higher accuracy, indicating overall better performance in prediction.\n",
    "- **Sensitivity:**\n",
    "Both MLP and DT achieve a sensitivity of 1.0 and 0.8 respectively, indicating that they correctly identify all actual positive instances.\n",
    "Preference: MLP and DT perform equally well in sensitivity.\n",
    "#### **Conclusion:**\n",
    "Based on the comparison, the MLP model would be preferred for predicting the risk of heart disease in patients. It achieves higher precision and accuracy compared to the DT model, indicating more accurate and reliable predictions overall. Additionally, both models perform equally well in sensitivity and specificity. Therefore, the MLP model would be the preferred choice for this task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
