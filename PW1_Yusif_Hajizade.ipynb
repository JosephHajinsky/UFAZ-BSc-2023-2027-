{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yusif Hajizade CS-020 id: 22022735\n",
    "# **Artificial Intelligence Practical Work 1**\n",
    "## **Lab: Iris dataset classification using a Decision Tree**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The objective of this lab is to implement a Decision Tree in order to classify Irises \n",
    "Specific objectives:\n",
    "- Observe the data, understand their nature and how to adapt them (if needed) so you can use them\n",
    "    in a Decision Tree model.\n",
    "- Understand how Decision Trees work so as to implement this model in a computer program.\n",
    "- Evaluate the results and put them into perspective with what we know about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1 The data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this lab, we will work on a classification problem (i.e. assign labels\n",
    "to data so as to group these data into distinct caterories).\n",
    "The initial dataset for machine learning is the Iris dataset, featuring three Iris species with 50 instances each. Characterized by four attributes (petal width, petal length, sepal width, and sepal length), it allows discrimination between species. Visualization, such as 2D scatterplots, simplifies the understanding of the 150 instances. Notably, Setosa is linearly separable, while distinguishing Versicolor and Virginica visually is more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Question:**\n",
    "- #### ***Is the Decision Tree model used for supervised or unsupervised classification? Explain your answer***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** **The Decision Tree model is used for supervised classification,**\n",
    "because the Decision Tree model is trained on a labeled dataset to predict the class labels for new instances, it falls under the category of supervised classification.\n",
    "\n",
    "- Supervised learning involves training a model on a labeled dataset, where the input data (features) is associated with corresponding output labels.\n",
    "- In the given dataset example, each instance has a set of features (SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm) and a corresponding label (Species), which indicates the class or category to which the instance belongs (e.g., Iris-setosa).\n",
    "- The objective of the Decision Tree model is to learn a mapping from the input features to the output labels based on the labeled training data.\n",
    "- The model is trained to make predictions on new, unseen instances by generalizing from the patterns observed in the training data.\n",
    "- In contrast, unsupervised learning involves tasks where the algorithm is given unlabeled data and needs to find patterns, relationships, or structure in the data without explicit guidance on the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  SepalLength  SepalWidth  PetalLength  PetalWidth      Species\n",
      "0   1          5.1         3.5          1.4         0.2  Iris-setosa\n",
      "1   2          4.9         3.0          1.4         0.2  Iris-setosa\n",
      "2   3          4.7         3.2          1.3         0.2  Iris-setosa\n",
      "3   4          4.6         3.1          1.5         0.2  Iris-setosa\n",
      "4   5          5.0         3.6          1.4         0.2  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "dataset_url = r\"C:\\Users\\User\\Desktop\\AI\\Iris.csv\"\n",
    "column_names = [\"Id\", \"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\", \"Species\"]\n",
    "\n",
    "df = pd.read_csv(dataset_url, names=column_names, header=0)\n",
    "\n",
    "# Display a sample of the dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2 Buidling a Decision Tree**\n",
    "\n",
    "In this section, we aim to build a Decision Tree for the Iris dataset, utilizing its four attributes to determine their discriminative power. The process involves identifying the attribute with the highest discriminative power and subsequently ranking the attributes. The goal is to establish a Decision Tree model that can predict the species of a new instance based on its attribute values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Preliminary Question:**\n",
    "- #### ***What is the nature of the attributes of the dataset?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the nature of the attributes is crucial for building an effective Decision Tree. It provides insights into the types of measurements and characteristics represented by the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  SepalLength  SepalWidth  PetalLength  PetalWidth      Species\n",
      "0   1          5.1         3.5          1.4         0.2  Iris-setosa\n",
      "1   2          4.9         3.0          1.4         0.2  Iris-setosa\n",
      "2   3          4.7         3.2          1.3         0.2  Iris-setosa\n",
      "3   4          4.6         3.1          1.5         0.2  Iris-setosa\n",
      "4   5          5.0         3.6          1.4         0.2  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "# Preliminary Question 1\n",
    "# What is the nature of the attributes of the dataset?\n",
    "\n",
    "# Display a sample of the dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: The attributes are numerical measurements related to the dimensions of Sepal and Petal.** They all describe atributes of Iris fower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### ***Do you think it is necessary to transform the attributes (scaling, standardization, ...)?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining the need for attribute transformation is crucial. Some machine learning algorithms, such as Decision Trees, are not sensitive to feature scaling, but it's important to consider for other algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminary Question 2\n",
    "# Do you think it is necessary to transform the attributes (scaling, standardization, ...)?\n",
    "X = df.drop([\"Id\", \"Species\"], axis=1)\n",
    "y = df[\"Species\"]\n",
    "\n",
    "# Define three partitions based on attribute values\n",
    "short_threshold = X.median() - X.std()\n",
    "long_threshold = X.median() + X.std()\n",
    "\n",
    "# Define three partitions based on attribute values using quantiles\n",
    "X[\"Partition\"] = pd.qcut(X[\"SepalLength\"], q=[0, 1/3, 2/3, 1], labels=[\"short\", \"average\", \"long\"])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.drop(\"Partition\", axis=1), y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** **It depends on the specific algorithm used. Decision Trees are not sensitive to feature scaling, feature scaling is necessary for Decision Trees**\n",
    "The code splits the dataset into training and testing sets without applying feature scaling. This is consistent with the observation that Decision Trees are not sensitive to feature scaling. \n",
    "In the context of Decision Trees, which is the algorithm used in the provided code, scaling or standardization of features is generally not necessary. Decision Trees make decisions based on the values of individual features and the splitting points selected during training. The algorithm is not influenced by the scale of the features. if your dataset involves other algorithms or models that are sensitive to feature scales (e.g., Support Vector Machines, k-Nearest Neighbors), scaling might be necessary. It's always a good practice to check the requirements of the specific algorithms you are using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### ***How are you going to use real value attributes to build your Decision Tree?***\n",
    "\n",
    "Understanding how to utilize real-value attributes is fundamental for Decision Tree construction. Decision Trees naturally handle real-value attributes, and this step explores how to leverage them in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        10\n",
      "Iris-versicolor       1.00      1.00      1.00         9\n",
      " Iris-virginica       1.00      1.00      1.00        11\n",
      "\n",
      "       accuracy                           1.00        30\n",
      "      macro avg       1.00      1.00      1.00        30\n",
      "   weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preliminary Question 3\n",
    "# How are you going to use real value attributes to build your Decision Tree?\n",
    "\n",
    "X = df.drop([\"Id\", \"Species\"], axis=1)\n",
    "y = df[\"Species\"]\n",
    "\n",
    "# Define three partitions based on attribute values\n",
    "short_threshold = X.median() - X.std()\n",
    "long_threshold = X.median() + X.std()\n",
    "\n",
    "# Define three partitions based on attribute values using quantiles\n",
    "X[\"Partition\"] = pd.qcut(X[\"SepalLength\"], q=[0, 1/3, 2/3, 1], labels=[\"short\", \"average\", \"long\"])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.drop(\"Partition\", axis=1), y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_str = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Answer: Decision Trees naturally handle real-value attributes for classification.**\n",
    "The code introduces three partitions based on the real-value attribute (SepalLength). The creation of categorical partitions facilitates the Decision Tree's ability to handle these attributes effectively. The Decision Tree rules provide insights into how the model makes predictions based on attribute values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### ***Have a look back at Fig. 2 p. 5: what relationship is there between this figure and the class repartitions of Table 1?***\n",
    "**Answer:** Figure 2 on page 5 visualizes the data in a 2D scatterplot, allowing us to observe the grouping of instances into different classes (species). The relationship with Table 1 lies in the correspondence between the visual representation and the class repartitions. The scatterplot visually represents how instances are distributed among different classes, while Table 1 quantifies this distribution for each group based on the discriminative attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### ***do you wish to continue adding branches in the tree? Explain your answer.***\n",
    "\n",
    "**Answer:** The decision to continue adding branches in the tree depends on the analysis of the current branches and their effectiveness in discriminating between classes. Further branches may lead to a more detailed and specific classification but could also risk overfitting the model to the training data. It's essential to strike a balance between model complexity and generalization to achieve optimal performance on new, unseen instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Continued Segmentation:**\n",
    "- #### ***Does the use of this attribute allow discrimination between classes?***\n",
    "**Answer: **The analysis of Table 2 presents the class repartitions for the attribute \"SW\" (sepal width). The distribution of instances among different classes in each group suggests whether this attribute effectively discriminates between classes.\n",
    "\n",
    "- #### ***Explain (and implement if you still have time) the procedure to continue segmenting the data.***\n",
    "\n",
    "Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (with SepalWidth): 1.00\n",
      "Classification Report (with SepalWidth):\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        10\n",
      "Iris-versicolor       1.00      1.00      1.00         9\n",
      " Iris-virginica       1.00      1.00      1.00        11\n",
      "\n",
      "       accuracy                           1.00        30\n",
      "      macro avg       1.00      1.00      1.00        30\n",
      "   weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Continue segmentation with another attribute (e.g., SepalWidth)\n",
    "X[\"Partition_SW\"] = pd.qcut(X[\"SepalWidth\"], q=[0, 1/3, 2/3, 1], labels=[\"short\", \"average\", \"long\"])\n",
    "X_train_SW, X_test_SW, y_train_SW, y_test_SW = train_test_split(X.drop([\"Partition\", \"Partition_SW\"], axis=1), y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a new Decision Tree classifier with the new attribute\n",
    "dt_classifier_SW = DecisionTreeClassifier()\n",
    "dt_classifier_SW.fit(X_train_SW, y_train_SW)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_SW = dt_classifier_SW.predict(X_test_SW)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy_SW = accuracy_score(y_test_SW, y_pred_SW)\n",
    "classification_report_str_SW = classification_report(y_test_SW, y_pred_SW)\n",
    "print(f\"Accuracy (with SepalWidth): {accuracy_SW:.2f}\")\n",
    "print(\"Classification Report (with SepalWidth):\\n\", classification_report_str_SW)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The code continues the segmentation process with the attribute \"SepalWidth.\" It creates new partitions and evaluates the Decision Tree classifier's performance with this additional attribute. This procedure can be repeated for other attributes to further refine the Decision Tree model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
